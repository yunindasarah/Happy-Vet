# -*- coding: utf-8 -*-
"""Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OLPSDy6CPgSCKwdb5AYVW7Rc-XeDDiaA
"""

import os
import matplotlib.pyplot as plt
import numpy as np
import tensorflow
from tensorflow.keras import layers
from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.applications.inception_v3 import preprocess_input
from tensorflow.keras.optimizers.legacy import Adam
from tensorflow.keras.layers import Input, Lambda, Dense, Flatten, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
import glob
from keras.models import load_model
import ntpath
from sklearn.metrics import confusion_matrix
from keras.callbacks import ModelCheckpoint, EarlyStopping
import pickle

from google.colab import drive
drive.mount('/content/drive/')

image_size = [224, 224]

training_path = os.listdir('/content/drive/MyDrive/Dataset/Dataset/Training')
testing_path = os.listdir('/content/drive/MyDrive/Dataset/Dataset/Testing')

inception = InceptionV3(input_shape = image_size + [3], include_top=False)

for layer in inception.layers:
    layer.trainable = False

trainfile = glob.glob('/content/drive/MyDrive/Dataset/Dataset/Training/*')
print(len(trainfile))

print("Creating Model")
model = Sequential([
    inception,
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(rate=0.2),
    Dense(len(trainfile), activation='softmax')
])


model.summary()

model.compile(
  loss='categorical_crossentropy',
  optimizer='adam',
  metrics=['accuracy']
)

training_datagen = ImageDataGenerator(rescale = 1./255,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)

testing_datagen = ImageDataGenerator(rescale = 1./255)

training_set = training_datagen.flow_from_directory('/content/drive/MyDrive/Dataset/Dataset/Training',
                                                 target_size = (224, 224),
                                                 batch_size = 50,
                                                 class_mode = 'categorical')

testing_set = testing_datagen.flow_from_directory('/content/drive/MyDrive/Dataset/Dataset/Testing',
                                            target_size = (224, 224),
                                            batch_size = 50,
                                            class_mode = 'categorical')

file = '/content/drive/MyDrive/Dataset/Dataset'
check = ModelCheckpoint(file, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')
early = EarlyStopping(monitor="accuracy", mode="max", patience=15)

callbacks_list = [check, early]

history = model.fit(
  training_set,
  validation_data=testing_set,
  epochs=20,
  steps_per_epoch=len(training_set),
  validation_steps=len(testing_set),
  callbacks=callbacks_list
)

accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(len(accuracy))

plt.plot(epochs, accuracy, 'r', "Training Accuracy")
plt.plot(epochs, val_accuracy, 'b', "Validation Accuracy")
plt.title('Training and Validation Accuracy')
plt.show()
print("")

plt.plot(epochs, loss, 'r', "Training Loss")
plt.plot(epochs, val_loss, 'b', "Validation Loss")
plt.title('Training and Validation Loss')
plt.show()
plt.show()
print("")

model.save("Model")

import tensorflow as tf
export_dir = 'saved_model/1'
tf.saved_model.save(model, export_dir = export_dir)

mode = "Speed"

if mode == 'storage':
  optimization = tf.lite.Optimize.OPTIMIZE_FOR_SIZE
elif mode == 'Speed':
  optimization = tf.lite.Optimize.OPTIMIZE_FOR_LATENCY
else:
  optimization = tf.lite.Optimize.DEFAULT

converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)

converter.optimizations = [optimization]

tflite_model = converter.convert();

import pathlib
tflite_model_file = pathlib.Path('./model.tflite')
tflite_model_file.write_bytes(tflite_model)